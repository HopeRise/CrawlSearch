{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91425eb2",
   "metadata": {},
   "source": [
    "## 1. Abstract\n",
    "- Brief summary\n",
    "- Objectives:\n",
    "  - Crawl web pages, extract content.\n",
    "  - Build an index for search.\n",
    "  - Enable ranked query results via Flask API.\n",
    "- Next steps:\n",
    "  - Improvements \n",
    "\n",
    "## 2. Overview\n",
    "- Solution Outline: Explain the full workflow:\n",
    "- Relevant Literature: Briefly cite research/articles on web crawling, TF-IDF, cosine similarity, or search systems.\n",
    "- Proposed System: Highlight architecture and its intended functionality.\n",
    "\n",
    "## 3. Design\n",
    "- System Capabilities:\n",
    "  - Max pages/depth crawling\n",
    "  - TF-IDF index construction\n",
    "  - Top-K query results\n",
    "- Interactions: How crawler, indexer, and query processor communicate.\n",
    "- Integration: File formats (JSON index, CSV queries), data flow.\n",
    "\n",
    "## 4. Architecture\n",
    "- Software Components:\n",
    "  - Scrapy spider\n",
    "  - Scikit-Learn indexer\n",
    "  - Flask query processor\n",
    "- Interfaces: APIs, endpoints, command-line interfaces.\n",
    "- Implementation Notes: Python version, libraries (Scrapy, sklearn, Flask)\n",
    "\n",
    "## 5. Operation\n",
    "- Installation Instructions: Dependencies, environment setup, pip/venv usage.\n",
    "- Software Commands:\n",
    "  - Running crawler\n",
    "  - Building the index\n",
    "  - Running Flask server and querying\n",
    "- Inputs: Seed URL, CSV queries, configuration options.\n",
    "\n",
    "## 6. Conclusion\n",
    "- Success/Failure:\n",
    "- Outputs:\n",
    "\n",
    "## 7. Data Sources\n",
    "- Links to websites crawled (if allowed).\n",
    "\n",
    "## 8. Test Cases\n",
    "- Framework: Pytest, unittest, or custom scripts.\n",
    "- Harness: How to test crawler, indexer, query processor.\n",
    "- Coverage: Edge cases like empty pages, invalid URLs, long queries, or spelling errors.\n",
    "\n",
    "## 9. Source Code\n",
    "- https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "## 10. Bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d1e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6827908",
   "metadata": {},
   "source": [
    "### Step 1: Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"pages\", exist_ok=True)\n",
    "\n",
    "class NotebookCrawler(scrapy.Spider):\n",
    "    name = \"notebook_crawler\"\n",
    "    \n",
    "    def __init__(self, seed_url, allowed_domain, max_pages, max_depth, *args, **kwargs):\n",
    "        super(NotebookCrawler, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [seed_url]\n",
    "        self.allowed_domains = [allowed_domain]\n",
    "        self.max_pages = max_pages\n",
    "        self.visited = set()\n",
    "        \n",
    "        # Set max depth in custom settings\n",
    "        self.custom_settings = {\n",
    "            'DEPTH_LIMIT': max_depth,\n",
    "            'AUTOTHROTTLE_ENABLED': True,\n",
    "            'LOG_ENABLED': True,\n",
    "            'CLOSESPIDER_PAGECOUNT': max_pages\n",
    "        }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Stop if max pages reached\n",
    "        if len(self.visited) >= self.max_pages:\n",
    "            self.logger.info(f\"Reached max pages limit: {self.max_pages}\")\n",
    "            return\n",
    "        \n",
    "        # Save HTML content\n",
    "        url_safe = response.url.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"_\")\n",
    "        with open(f\"pages/{url_safe}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        self.visited.add(response.url)\n",
    "        self.logger.info(f\"Saved page {len(self.visited)}/{self.max_pages}: {response.url}\")\n",
    "        \n",
    "        # Extract and follow links\n",
    "        if len(self.visited) < self.max_pages:\n",
    "            links = LinkExtractor(allow_domains=self.allowed_domains).extract_links(response)\n",
    "            for link in links:\n",
    "                if link.url not in self.visited:\n",
    "                    yield response.follow(link.url, self.parse)\n",
    "\n",
    "# Usage with configurable parameters\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "})\n",
    "\n",
    "process.crawl(\n",
    "    NotebookCrawler,\n",
    "    seed_url='https://quotes.toscrape.com',      \n",
    "    allowed_domain='quotes.toscrape.com',      \n",
    "    max_pages=100,                                  \n",
    "    max_depth=5                                   \n",
    ")\n",
    "try:\n",
    "    process.start()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bb678",
   "metadata": {},
   "source": [
    "### Step 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a266a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
