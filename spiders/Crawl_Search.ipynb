{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91425eb2",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This project focuses on designing and implementing a web-based search pipeline, including document \n",
    "acquisition, indexing, and query retrieval. A Scrapy-powered crawler is used to download and store web documents in \n",
    "HTML format. An inverted index is then generated using TF-IDF weighting to support efficient similarity-based \n",
    "retrieval. Finally, a Flask-driven query processor enables users to submit free-text searches and receive ranked \n",
    "document results based on cosine similarity.\n",
    "\n",
    "Future enhancements can focus on improving scalability and retrieval quality. Optional features such as concurrent \n",
    "and distributed crawling using AutoThrottle and Scrapyd would allow faster and broader document collection. Search \n",
    "accuracy could be increased by integrating semantic vector embeddings like Word2Vec or FAISS-based k-nearest neighbor \n",
    "similarity. Additionally, a front-end search interface and production deployment would make the system more \n",
    "user-accessible and robust.\n",
    "\n",
    "## Overview\n",
    "The project focus on web document retrieval and query processing. It consists\n",
    "of three compononent: a web crawler, an indexer, and a query procesor. These\n",
    "components works together to enable efficiant crawling, indexing, and \n",
    "querying of web docuemnts. \n",
    "\n",
    "### Soultion Outline\n",
    "1. **Web Crawler**: Use Scrapy to roam the web and download web documents.\n",
    "2. **Indexer**: Build an inverted index using the TF-IDF and Cosine similarities to search and retrieve \n",
    "3. **Query Processor**: Accepts user queries and returns ranked document results using TF-IDF and cosine similarity.\n",
    "\n",
    "### Relevant Literature: \n",
    "- [Scrapy Documentation](https://docs.scrapy.org/en/latest/intro/tutorial.html):  \n",
    "  This official tutorial provides a comprehensive introduction to Scrapy, a powerful Python framework for web crawling and scraping. It covers spider creation, data extraction, and best practices for scalable web data collection, which directly informs the design of the project's web crawler component.\n",
    "\n",
    "- [Flask Documentation](https://flask.palletsprojects.com/en/stable):  \n",
    "  The Flask documentation offers detailed guidance on building lightweight web applications and APIs in Python. It is the primary reference for implementing the query processor and REST API endpoints, enabling user interaction and search result delivery in this project.\n",
    "\n",
    "For the search indexing section, I have already studied and applied established techniques, as demonstrated in this project.\n",
    "\n",
    "### Proposed System\n",
    "The system use the power of scrapy for web crawling, Skit-Learn for TF-IDF indexing and Cosine similarities, and Flask for query processing. The combination of these technologies, it delivers a solution for web docuement retrieval and query processing. \n",
    "\n",
    "## Design\n",
    "The system's capabilities includes downloaading web documents, constucting an inverted index, and processing user queries. it involves the Scrapy crawler to collect web documents, th indexer building the TF-IDF matrix, and the Flask processor handeling user queries. Integration is achieved through data exchange between components and adherence to defined interfaces.\n",
    "\n",
    "![Web Search Engine Diagram](Assets/Web_Search_Engine_Diagram.png)\n",
    "\n",
    "This activity diagram outlines the flow of activities within the system, including crawling, indexing, processing queries, and returning results. Each \n",
    "component interacts with the others to achieve the overall functionality of the system.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The architecture of the system involves three main components: the Scrapy crawler, the inverted indexer, and the Flask query processor. These components interact with each other to enable web document crawling, indexing, and query processing.\n",
    "\n",
    "![Architecture Diagram](Assets/Architecture_Diagram.png)\n",
    "\n",
    "This diagram illustrates the interaction between the components and how they utilize interfaces such as file I/O and HTTP endpoints for communication and data exchange. The implementation relies on external libraries such as Scikit-Learn, BeutifulSoup, Spider, and Flask to enable various functionality.\n",
    "\n",
    "## Operation\n",
    "\n",
    "**Installation Instructions:**  \n",
    "  - Ensure Python 3.8 or higher is installed.  \n",
    "  - Create and activate a virtual environment:  \n",
    "    - `python -m venv venv`  \n",
    "    - `venv\\Scripts\\activate` (Windows) or `source venv/bin/activate` (Linux/Mac)  \n",
    "  - Install all required dependencies using:  \n",
    "    - `pip install -r requirements.txt`  \n",
    "\n",
    "**Software Commands:**  \n",
    "  - **Running the crawler:** Execute the relevant notebook cells or script to start the Scrapy crawler.  \n",
    "  - **Building the index:** Run the indexing cells or script to generate the inverted index (`index.json`).  \n",
    "  - **Running the Flask server and querying:** Start the Flask server by running the appropriate notebook cells or script. Submit queries via the web interface or batch CSV upload.\n",
    "\n",
    "**Inputs:**  \n",
    "  - **Seed URL:** The initial URL for the crawler.  \n",
    "  - **CSV queries:** A CSV file containing `query_id` and `query_text` columns for batch search.  \n",
    "  - **Configuration options:** Parameters such as maximum pages to crawl, crawl depth, and allowed domains.\n",
    "\n",
    "**Outputs:**  \n",
    "  - **Crawled HTML files:** Stored in the `pages/` directory.  \n",
    "  - **Cleaned text files:** Stored in the `cleaned_text/` directory.  \n",
    "  - **Inverted index:** Saved as `index.json`.  \n",
    "  - **Search results:** Ranked results provided as a downloadable CSV file via the Flask API.\n",
    "\n",
    "## Conclusion\n",
    "- **Success/Failure:**  \n",
    "  The project successfully demonstrates the implementation of a modular web search pipeline, encompassing web crawling, document cleaning, inverted indexing, and query processing. The system is capable of acquiring web documents, extracting and cleaning their content, constructing a positional inverted index, and retrieving relevant documents in response to user queries using TF-IDF and cosine similarity. Error handling is incorporated to manage missing files or invalid queries, ensuring robust operation.\n",
    "\n",
    "- **Outputs:**  \n",
    "  The outputs of the system are as follows:  \n",
    "  - Crawled HTML files are stored in the `pages/` directory.  \n",
    "  - Cleaned text files are saved in the `cleaned_text/` directory.  \n",
    "  - The positional inverted index is generated and saved as `index.json`.  \n",
    "  - Search results for user queries are provided as ranked CSV files, which can be downloaded via the Flask API.  \n",
    "  - Example outputs, including ranked document results and similarity scores for sample queries, are displayed below as evidence of successful retrieval and ranking.\n",
    "\n",
    "## Data Sources\n",
    "- Web documents are sourced from [quotes.toscrape.com](https://quotes.toscrape.com)\n",
    "- Additional data sources can be integrated as needed for testing and evaluation.\n",
    "\n",
    "## Test Cases\n",
    "Test cases involve validating the functionality of the crawler, indexer, and query processor. Frameworks such as Scrapy's testing tools and unit testing libraries for Python can be utilized. Test coverage includes scenarios for crawling, indexing, and querying.\n",
    "\n",
    "## Source Code\n",
    "The complete source code for this project, including the web crawler, indexer, and query processor, is provided in the accompanying Jupyter notebook (`Crawl_Search.ipynb`) and related Python scripts within the project directory. All implementation details, code cells, and configuration files are included to enable full reproducibility of the results.\n",
    "\n",
    "Key files and directories:\n",
    "- `Crawl_Search.ipynb`: Main notebook containing the code for crawling, indexing, and query processing.\n",
    "- `pages/`: Directory where crawled HTML files are stored.\n",
    "- `cleaned_text/`: Directory containing cleaned text files extracted from HTML pages.\n",
    "- `index.json`: Generated inverted index used for search and retrieval.\n",
    "- `requirements.txt`: List of required Python packages for the project.\n",
    "\n",
    "To access or modify the source code, open the notebook or scripts in your preferred Python environment (e.g., Jupyter Notebook or Visual Studio Code).\n",
    "\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] Scrapy, “Scrapy Tutorial — Scrapy 2.3.0 documentation,” 2012. [Online]. Available: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "[2] L. Richardson, “Beautiful Soup Documentation — Beautiful Soup 4.4.0 Documentation,” 2015. [Online]. Available: https://beautiful-soup-4.readthedocs.io/en/latest\n",
    "\n",
    "[3] W3Schools, “Python JSON,” 2025. [Online]. Available: https://www.w3schools.com/python/python_json.asp\n",
    "\n",
    "[4] Scikit-learn, “sklearn.metrics.pairwise.cosine_similarity,” 2018. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "[5] Scikit-learn, “TfidfVectorizer,” 2018. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "[6] Flask, “Welcome to Flask — Flask Documentation (3.1.x),” 2025. [Online]. Available: https://flask.palletsprojects.com/en/stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d1e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6827908",
   "metadata": {},
   "source": [
    "### Step 1: Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccbc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:23:35 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: scrapybot)\n",
      "2025-12-02 15:23:35 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '6.0.2',\n",
      " 'libxml2': '2.11.9',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '25.5.0',\n",
      " 'Python': '3.14.0 (tags/v3.14.0:ebf955d, Oct  7 2025, 10:15:03) [MSC v.1944 '\n",
      "           '64 bit (AMD64)]',\n",
      " 'pyOpenSSL': '25.3.0 (OpenSSL 3.5.4 30 Sep 2025)',\n",
      " 'cryptography': '46.0.3',\n",
      " 'Platform': 'Windows-11-10.0.26200-SP0'}\n",
      "2025-12-02 15:23:35 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-12-02 15:23:35 [scrapy.extensions.telnet] INFO: Telnet Password: 30164f11bb523482\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-12-02 15:23:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 'INFO'}\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:23:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-12-02 15:23:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-12-02 15:23:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-12-02 15:23:38 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-12-02 15:23:38 [notebook_crawler] INFO: Total Documents Scraped: 254\n",
      "2025-12-02 15:23:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 129050,\n",
      " 'downloader/request_count': 323,\n",
      " 'downloader/request_method_count/GET': 323,\n",
      " 'downloader/response_bytes': 1361266,\n",
      " 'downloader/response_count': 323,\n",
      " 'downloader/response_status_count/200': 254,\n",
      " 'downloader/response_status_count/308': 69,\n",
      " 'dupefilter/filtered': 877,\n",
      " 'elapsed_time_seconds': 3.771481,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 12, 2, 21, 23, 38, 969220, tzinfo=datetime.timezone.utc),\n",
      " 'items_per_minute': 0.0,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 5,\n",
      " 'response_received_count': 254,\n",
      " 'responses_per_minute': 5080.0,\n",
      " 'scheduler/dequeued': 323,\n",
      " 'scheduler/dequeued/memory': 323,\n",
      " 'scheduler/enqueued': 323,\n",
      " 'scheduler/enqueued/memory': 323,\n",
      " 'start_time': datetime.datetime(2025, 12, 2, 21, 23, 35, 197739, tzinfo=datetime.timezone.utc)}\n",
      "2025-12-02 15:23:38 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"pages\", exist_ok=True)\n",
    "\n",
    "class NotebookCrawler(scrapy.Spider):\n",
    "    name = \"notebook_crawler\"\n",
    "    \n",
    "    def __init__(self, seed_url, allowed_domain, max_pages, max_depth, *args, **kwargs):\n",
    "        super(NotebookCrawler, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [seed_url]\n",
    "        self.allowed_domains = [allowed_domain]\n",
    "        self.max_pages = max_pages\n",
    "        self.visited = set()\n",
    "        \n",
    "        # Set max depth in custom settings\n",
    "        self.custom_settings = {\n",
    "            'DEPTH_LIMIT': max_depth,\n",
    "            'AUTOTHROTTLE_ENABLED': True,\n",
    "            'LOG_ENABLED': True,\n",
    "            'CLOSESPIDER_PAGECOUNT': max_pages\n",
    "        }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        page_uuid = str(uuid.uuid4()).upper() + \".html\"\n",
    "        \n",
    "        with open(f\"pages/{page_uuid}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        self.visited.add(response.url)\n",
    "\n",
    "        # Continue crawling until max_pages reached\n",
    "        if len(self.visited) < self.max_pages:\n",
    "            links = LinkExtractor(allow_domains=self.allowed_domains).extract_links(response)\n",
    "            for link in links:\n",
    "                if link.url not in self.visited:\n",
    "                    yield response.follow(link.url, self.parse)\n",
    "\n",
    "    # Only print once when crawler is done\n",
    "    def closed(self, reason):\n",
    "        self.logger.info(f\"Total Documents Scraped: {len(self.visited)}\")\n",
    "\n",
    "\n",
    "# Crawler run\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "})\n",
    "\n",
    "process.crawl(\n",
    "    NotebookCrawler,\n",
    "    seed_url='https://quotes.toscrape.com',\n",
    "    allowed_domain='quotes.toscrape.com',\n",
    "    max_pages=100,\n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "try:\n",
    "    process.start()\n",
    "except:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bb678",
   "metadata": {},
   "source": [
    "### Step 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a266a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 01A378E4-DF5E-4A2D-9D43-3089864F689D.html (332 characters)\n",
      "Processed: 04141F94-F3EB-4490-92B2-993968E97145.html (466 characters)\n",
      "Processed: 0475044E-5B2B-4727-9ED8-0D9DE2934E8C.html (352 characters)\n",
      "Processed: 047F371D-2444-4CE4-8DA0-D44B54F26954.html (564 characters)\n",
      "Processed: 082BF3C2-DE6A-4DCF-BBA1-8CAB9B453018.html (424 characters)\n",
      "Processed: 093A3E73-8E61-49D3-B379-596587BB47B0.html (2625 characters)\n",
      "Processed: 09FAD845-72AE-4762-8428-6BCDA51192D6.html (1290 characters)\n",
      "Processed: 0A56D6A0-A1BD-44C2-BFE7-A22C0FD35B06.html (337 characters)\n",
      "Processed: 0B4D9B57-7654-48B6-B71F-2D5B7E562BDD.html (929 characters)\n",
      "Processed: 0BF4A514-E4F2-47DC-8BF6-B430FB574667.html (1362 characters)\n",
      "Processed: 0CF813DF-4356-41DB-A57F-8C5ADC38FE5C.html (505 characters)\n",
      "Processed: 0DA76422-66F6-4F70-AF40-AE56DF1DAB5B.html (1481 characters)\n",
      "Processed: 110BE0AA-0FDF-456E-ADC8-F84E3CADDEA5.html (371 characters)\n",
      "Processed: 116EEFFD-0727-4202-A952-A29B2B5D9843.html (307 characters)\n",
      "Processed: 12FFE4D9-6A76-4556-82B2-A63FC2222AFF.html (994 characters)\n",
      "Processed: 1322F688-CE63-4F5F-9FBB-AF25234494D3.html (476 characters)\n",
      "Processed: 1349C14B-AB47-4564-BBE7-10F09CB968E0.html (428 characters)\n",
      "Processed: 143789D4-3683-4587-94A5-0A4E9B803CCC.html (564 characters)\n",
      "Processed: 152956CF-3261-47C0-9FA8-40F6819698F3.html (319 characters)\n",
      "Processed: 155BADCA-2EA2-4984-8A38-5E26F50E7770.html (345 characters)\n",
      "Processed: 15A17A57-3949-4E53-A939-DBEA0CE2A4B5.html (465 characters)\n",
      "Processed: 177F1980-7693-4CB8-8CC5-B1E980E91FEC.html (868 characters)\n",
      "Processed: 1816804F-32CF-43BB-91E4-68623270661D.html (397 characters)\n",
      "Processed: 18E0C7EA-2C54-48C2-8225-BB98CF303EED.html (2843 characters)\n",
      "Processed: 18F00FEE-81AE-432C-857F-1E9F44C80897.html (620 characters)\n",
      "Processed: 19E27C6C-7712-4A11-A67A-7DC4AD9635C2.html (1000 characters)\n",
      "Processed: 19F8E359-0F90-48E9-8FE8-9BCACAE395ED.html (1794 characters)\n",
      "Processed: 1B24C8FC-8DA1-4AC5-ACF7-C81E0B225F84.html (3967 characters)\n",
      "Processed: 1C450A87-A7E5-466B-B6EC-A55B885C2DCD.html (2674 characters)\n",
      "Processed: 1EA01E52-7926-4A71-92AA-8BC0F91049D9.html (2375 characters)\n",
      "Processed: 1FD6A49B-DA81-42CA-B244-8CCE3C324B62.html (600 characters)\n",
      "Processed: 22157036-B266-4890-A66A-225CD7043E03.html (352 characters)\n",
      "Processed: 227B25D2-1BD2-49E9-AAD9-FC6BC19BC4DC.html (360 characters)\n",
      "Processed: 229022EB-0D6B-4AB8-A678-C837378B4C6F.html (319 characters)\n",
      "Processed: 2352B10E-E2F3-4AE7-8788-7C39AEF3301B.html (1145 characters)\n",
      "Processed: 2450D4D6-6873-473B-B33A-BF73707A4CC6.html (2843 characters)\n",
      "Processed: 24671B39-E369-4D73-BD9E-B7703BF2101B.html (3707 characters)\n",
      "Processed: 256E61D8-33A4-4BC3-A7F0-7945C195EA03.html (594 characters)\n",
      "Processed: 2577C9BD-E33E-485E-8F33-13CD7C3E0B52.html (2148 characters)\n",
      "Processed: 25D44D07-DD14-4ECA-9798-EF8BE8E59725.html (522 characters)\n",
      "Processed: 2643026D-8A95-4B6F-A233-8E8029921E17.html (340 characters)\n",
      "Processed: 2957A5A0-7389-476E-A570-084BFE1312FA.html (505 characters)\n",
      "Processed: 2A38CD67-658C-46EB-B447-BECD2960EC47.html (1681 characters)\n",
      "Processed: 2A93FB6F-8536-4D69-A297-22B464280267.html (1936 characters)\n",
      "Processed: 2AA01531-2643-47D5-8615-67595CCDF07F.html (1000 characters)\n",
      "Processed: 2B24FD2C-6985-4BA2-BA50-F20504B9198D.html (328 characters)\n",
      "Processed: 2B577D7F-0C33-492B-BDC2-8032F9735EC5.html (332 characters)\n",
      "Processed: 2DB02180-39C1-4BDB-B18C-0A0058F8A4BB.html (520 characters)\n",
      "Processed: 2E1F2F52-8BEE-497F-8383-2DE4D69A7FCF.html (1447 characters)\n",
      "Processed: 2E860DD4-A046-4412-8635-7E506CDCDEE6.html (1000 characters)\n",
      "Processed: 319592F8-8DC0-4318-90BE-18A0FB068A51.html (295 characters)\n",
      "Processed: 327095A8-938F-426C-A7C2-70EE2F31FA79.html (413 characters)\n",
      "Processed: 33F4141D-3201-419A-874A-DD5691B9C416.html (1447 characters)\n",
      "Processed: 3533AFDC-B319-4440-A207-0EB8CF3441E3.html (448 characters)\n",
      "Processed: 36A37D6D-E66F-4E77-AFA6-2E9863886156.html (1860 characters)\n",
      "Processed: 3752EA3F-E2DA-42C4-997B-7FAC67D2D055.html (373 characters)\n",
      "Processed: 38060306-3510-44F1-B7BE-DBD18FEC9EE9.html (397 characters)\n",
      "Processed: 38DC2A23-98E6-4AEE-9F7C-E078BEEE97D6.html (355 characters)\n",
      "Processed: 3922FBD1-60CC-4842-85BD-EAF0191727A0.html (326 characters)\n",
      "Processed: 3B101EFA-D18C-4DA5-AD0C-E9B14DC827EF.html (335 characters)\n",
      "Processed: 3B27E9ED-7A9C-4D08-98DB-ED8BAE8BF0C4.html (296 characters)\n",
      "Processed: 3B44AE82-B694-4842-B487-6EEA2D1568D9.html (1646 characters)\n",
      "Processed: 3CABD456-4F5E-443A-8C52-2EF18CC89150.html (313 characters)\n",
      "Processed: 3D1349EF-A4BF-46C5-AB82-BC0791600639.html (1597 characters)\n",
      "Processed: 3E92214F-6341-4E24-98A4-9B7AF56AF587.html (676 characters)\n",
      "Processed: 4008D698-2B91-4080-B169-57CBB59F6CDA.html (1226 characters)\n",
      "Processed: 4015184C-06FA-4F2E-A8A3-C42F66D28AEA.html (332 characters)\n",
      "Processed: 4191B102-ED93-4FB7-9295-98ECC87E13A1.html (476 characters)\n",
      "Processed: 41CB6A86-FF93-43A2-9033-82BEC871CFBE.html (356 characters)\n",
      "Processed: 426D55D9-597C-4C33-AB49-01FB59612CDC.html (315 characters)\n",
      "Processed: 43C1C0EF-D837-483F-9B77-E6A1B5F4A568.html (1794 characters)\n",
      "Processed: 445767AF-4C3A-4E72-BC18-F57BF4FCAB04.html (317 characters)\n",
      "Processed: 448B38DA-81F8-42AD-8189-27F9DEF85A16.html (2031 characters)\n",
      "Processed: 452428B0-B872-4772-8D1A-3239F4C33B8D.html (399 characters)\n",
      "Processed: 469ACEC1-92DA-411F-B8F6-0CC50FF31FCC.html (1936 characters)\n",
      "Processed: 4726A964-A1D1-4312-A514-43BF12DDD7B4.html (2822 characters)\n",
      "Processed: 496E86A2-136E-4FBD-BB36-E081DAF7E24A.html (397 characters)\n",
      "Processed: 49B11B3B-1B50-43D1-8728-FD5E81496F7F.html (1264 characters)\n",
      "Processed: 4B319335-0FD8-4AEC-91F9-11D6DD2E8A0C.html (356 characters)\n",
      "Processed: 4E45C905-61D0-4809-AA19-01024EBFAA48.html (330 characters)\n",
      "Processed: 4E638282-3800-49BA-A2C5-4E9BD45850AA.html (368 characters)\n",
      "Processed: 510F15D1-5C8B-475D-BC5A-20FE9A04D234.html (321 characters)\n",
      "Processed: 53477293-9075-4EF3-B04C-197B325F5411.html (1681 characters)\n",
      "Processed: 552723D0-827F-4C63-9F9F-F089CEC426C3.html (335 characters)\n",
      "Processed: 5577E40E-D06A-4A90-9E67-DB124B8AF34B.html (564 characters)\n",
      "Processed: 562F1830-0530-49C2-9C44-B29EEAA49080.html (377 characters)\n",
      "Processed: 564797C5-CF5F-4856-B107-B0B5B54EFC1D.html (393 characters)\n",
      "Processed: 598E6EBB-95B2-4870-96CD-C5EE4F9ACD82.html (3743 characters)\n",
      "Processed: 5B58F155-F2E1-4BAF-A0DC-9F3949DA12F7.html (3936 characters)\n",
      "Processed: 5BF79FED-9B33-4C09-8168-2D8379B1D3D1.html (1768 characters)\n",
      "Processed: 5C04453C-54A2-4591-B1A3-763B8FA24FFE.html (411 characters)\n",
      "Processed: 5C21BF26-8659-41C4-B995-E2DEABDFAE39.html (751 characters)\n",
      "Processed: 5DB981C2-555A-4CFA-932E-3AB7E61AD609.html (1781 characters)\n",
      "Processed: 5E25219F-CCCD-47DA-BC52-B93E4628E59E.html (409 characters)\n",
      "Processed: 5F944E70-E613-498D-BAB5-5D32FA109483.html (3707 characters)\n",
      "Processed: 5FDAAF48-B41E-4BC9-B1CA-0DFC2A1533CC.html (364 characters)\n",
      "Processed: 6086BB1E-C0DC-4592-B486-30EB431C8415.html (1936 characters)\n",
      "Processed: 627384DD-CD51-441C-ADA1-25A12B4887DF.html (1976 characters)\n",
      "Processed: 63745780-A26E-4513-A419-C1F55FA1997B.html (385 characters)\n",
      "Processed: 65CA66E8-7540-4766-9A7E-F2EE9359BA2A.html (357 characters)\n",
      "Processed: 66984703-9663-4631-918B-33442A2DA3A2.html (389 characters)\n",
      "Processed: 66C61727-1376-4A5D-B4FD-2DA37954AF72.html (352 characters)\n",
      "Processed: 6719D5DD-3085-449B-9960-A5384EB6B673.html (391 characters)\n",
      "Processed: 6802B16B-7026-4604-AF39-AA0756762A89.html (448 characters)\n",
      "Processed: 689586C1-40ED-491E-9775-ECC7E211C967.html (363 characters)\n",
      "Processed: 68AEFC77-FB2A-4E5B-946E-3C12741F00DE.html (2880 characters)\n",
      "Processed: 6949D70C-A997-4C4D-8EE9-7C3350890D8E.html (2141 characters)\n",
      "Processed: 69ED70D5-F54F-4955-AECF-17977750BDB5.html (356 characters)\n",
      "Processed: 69FD6200-4A89-42FD-ABFF-4AD4E0BF7EEC.html (2843 characters)\n",
      "Processed: 6A384626-F517-4F43-83C9-BB62E57C5093.html (3707 characters)\n",
      "Processed: 6B5FE92C-8B7A-4B67-9D3E-40D2DCAD7A4F.html (397 characters)\n",
      "Processed: 6BBA9C0E-60AB-497B-A1D2-736183958415.html (751 characters)\n",
      "Processed: 6BCE0B45-98D9-4C4E-A3D0-8CD8EF493C86.html (1673 characters)\n",
      "Processed: 6CE8C7C6-9315-4715-A00D-2CE7302F6353.html (868 characters)\n",
      "Processed: 6D6A68B7-6F46-486B-939B-1D1BC3EE4B46.html (380 characters)\n",
      "Processed: 6DB28EDC-9BD8-4345-994F-7B3096A5541B.html (102 characters)\n",
      "Processed: 6E9D9740-0A8F-40A4-A68A-5E308AE6E46E.html (367 characters)\n",
      "Processed: 70E674DE-085A-400B-BE2A-849DDB031CC8.html (378 characters)\n",
      "Processed: 712E6738-202E-4B8D-AD17-61D32CF07110.html (318 characters)\n",
      "Processed: 72254FD1-8E9F-4BDF-AD2B-357DE54BAF9E.html (373 characters)\n",
      "Processed: 728FBB7D-DC27-4284-B3D3-FDE88C25E792.html (321 characters)\n",
      "Processed: 7656C754-86A4-450B-9BBC-6057D6DB22A7.html (282 characters)\n",
      "Processed: 76671C14-160A-4D50-AB0C-FD2B2FCBB844.html (330 characters)\n",
      "Processed: 7802DDB8-D80C-4688-BE10-B6D5EB9F14A9.html (428 characters)\n",
      "Processed: 7C23BFB8-9AA9-47CF-8A89-E0BABD535F2E.html (334 characters)\n",
      "Processed: 7C4BDD85-D88A-47A5-9A88-5ABDDCF65395.html (505 characters)\n",
      "Processed: 7D4D9400-D197-44D3-A1A5-81E3FFC32E3B.html (307 characters)\n",
      "Processed: 7E2AD24A-8E08-4C9F-93ED-40475BC17E2B.html (353 characters)\n",
      "Processed: 7F09C32E-9323-44EE-A0E5-4845C6672D57.html (364 characters)\n",
      "Processed: 7F3741AD-B245-4C4C-9F58-710F294B1C2D.html (400 characters)\n",
      "Processed: 7F38D6B0-1318-4227-A003-7447A9DCF320.html (1740 characters)\n",
      "Processed: 7F47B91C-F5F4-4AF9-9F88-54EE0A226984.html (522 characters)\n",
      "Processed: 81430200-D444-4FBC-AF73-549C46BB2BC9.html (1003 characters)\n",
      "Processed: 816490E7-F72C-4848-AFB3-4BE4FD5DD981.html (363 characters)\n",
      "Processed: 81892F47-6B53-4C44-90C3-F5419AA6D7B6.html (1000 characters)\n",
      "Processed: 82DBC2EC-F158-4CC2-817B-868DCD2BEB7F.html (2115 characters)\n",
      "Processed: 868387F2-24F5-49C5-A03F-0189E170620D.html (452 characters)\n",
      "Processed: 8686B71C-605E-471A-94DA-359BC819AA67.html (1325 characters)\n",
      "Processed: 86D6587D-76B0-45B9-9F67-65B5EEEDE86D.html (479 characters)\n",
      "Processed: 88E48F82-C9DA-48BE-A7E2-14DB57BF1AAB.html (336 characters)\n",
      "Processed: 8D4CBAD3-13E4-4351-834B-D6D9DEA3FBDE.html (494 characters)\n",
      "Processed: 8F522E92-4632-482B-9B04-5A63E4A25682.html (751 characters)\n",
      "Processed: 8F553460-8AB1-46D4-AF72-05D57CF1CDAA.html (408 characters)\n",
      "Processed: 8FBF8645-56B8-4A4F-AFE2-93C9306715E9.html (279 characters)\n",
      "Processed: 90A299A4-6F1B-4985-A70A-6F30FE611683.html (2262 characters)\n",
      "Processed: 90EC2EE5-BFF6-47B9-A552-0696EEED0CE5.html (2880 characters)\n",
      "Processed: 91A86C84-4EA7-4218-B5AD-EB2C0C25948A.html (298 characters)\n",
      "Processed: 930DF387-77C0-452E-B00D-82B59A20F1A0.html (1447 characters)\n",
      "Processed: 9352F70F-7394-4742-B5E7-3F60F20B58C6.html (1299 characters)\n",
      "Processed: 94B918C4-C2B8-4A8E-A1A4-0537C216FB85.html (508 characters)\n",
      "Processed: 9564DC0D-B724-4F1D-A3AD-EB7A22EC4110.html (508 characters)\n",
      "Processed: 96226E88-38F2-4887-9325-49D170AC3A30.html (351 characters)\n",
      "Processed: 96D97639-E54A-47DA-B624-ED50D98D5C12.html (426 characters)\n",
      "Processed: 9756D384-87C9-43B9-ABC9-E654A69E58E2.html (1673 characters)\n",
      "Processed: 9A9CC900-AD2A-44D8-83D9-E7ED328F3A02.html (352 characters)\n",
      "Processed: 9B0C7B29-522B-46CB-AB84-4C5D9C6ECF37.html (351 characters)\n",
      "Processed: 9B6A7C0F-89B1-4416-A5C5-B06927720B99.html (1130 characters)\n",
      "Processed: 9C56B5E0-9827-4D1D-AF0E-0512EA78748A.html (401 characters)\n",
      "Processed: 9CAC90A2-9AC7-44BF-A289-B418FF7330EE.html (1673 characters)\n",
      "Processed: 9D90DF52-C203-42F0-9657-A4E56AA18667.html (356 characters)\n",
      "Processed: 9E0B1160-8555-4697-A973-3CC71094990F.html (401 characters)\n",
      "Processed: A02977AC-1BE1-4C08-8437-A7D11BF43BC3.html (2880 characters)\n",
      "Processed: A06344E2-B3B6-47CC-96D3-CAE95B107A04.html (1768 characters)\n",
      "Processed: A12123FF-A300-4873-9911-521151EEA9BE.html (620 characters)\n",
      "Processed: A22EF9AC-9847-421F-8384-0F429B7B0B91.html (647 characters)\n",
      "Processed: A25F3D9E-38EE-432B-9F9C-8BE2655D2E0A.html (322 characters)\n",
      "Processed: A26E4AC7-158B-47FD-9D3C-F2EDEEC12F9D.html (2457 characters)\n",
      "Processed: A2AE684B-49E0-43ED-9EE6-75AB5A8E1AB8.html (1673 characters)\n",
      "Processed: A2D443A2-058A-4F13-B101-A74A85CA65C2.html (3462 characters)\n",
      "Processed: A2E09126-7AE3-4BD8-B050-58301395AB1F.html (1711 characters)\n",
      "Processed: A2F674D9-094B-4F5E-A6F9-8D8034CE4B27.html (465 characters)\n",
      "Processed: A3540090-991F-4AA1-B15F-6842D0019190.html (340 characters)\n",
      "Processed: A379EA71-0B74-41D5-B528-B7EFDA1EE095.html (424 characters)\n",
      "Processed: A499EB41-D47A-494C-AD9A-BA798A01A001.html (318 characters)\n",
      "Processed: A88832BC-D1AF-4FA4-909B-7C49FD394C75.html (1365 characters)\n",
      "Processed: A8A57473-EB41-41DD-A766-7F2E3B5194B3.html (3981 characters)\n",
      "Processed: A8F5C839-4A4A-4E6E-A5EF-FBB26C6C94AE.html (279 characters)\n",
      "Processed: A9E761AF-E53B-42FC-BF20-F97D35B8096E.html (2129 characters)\n",
      "Processed: AA01085D-B98E-4C85-8526-C1708EA85167.html (410 characters)\n",
      "Processed: AAE268AD-1987-4702-BBF3-263746EF9D42.html (511 characters)\n",
      "Processed: AAF2749B-A5F3-428D-AADF-BBFD0CEC7656.html (555 characters)\n",
      "Processed: ABD65067-22D6-49D6-993C-7316D04A3C6C.html (1512 characters)\n",
      "Processed: AC281E00-82F7-44DF-AB5B-F65342539C98.html (3707 characters)\n",
      "Processed: AE2E0321-FF42-4E05-9745-883C0CCA4A8F.html (393 characters)\n",
      "Processed: AF396038-8321-43F4-B52F-C29B93F4B9A8.html (317 characters)\n",
      "Processed: B0104148-C5F2-409E-A4C7-641FA313D262.html (2293 characters)\n",
      "Processed: B3064153-C5AC-44C2-A23D-17AF6E880B7F.html (331 characters)\n",
      "Processed: B36A7A4F-63DD-4674-9FCD-C70359C688AB.html (513 characters)\n",
      "Processed: B4A91DB3-7656-4647-AB33-4AB67F6412E8.html (3462 characters)\n",
      "Processed: B55779DF-09EF-49ED-95D3-F66203456639.html (377 characters)\n",
      "Processed: B56ADBFC-17B8-42AF-AB3D-FA005986C3CE.html (512 characters)\n",
      "Processed: B5C7EC90-69F5-4899-981A-BE4A4F15898F.html (296 characters)\n",
      "Processed: B672152A-DBC0-4272-AFE6-14270837E28B.html (1125 characters)\n",
      "Processed: B69B2594-5BF6-45D9-98AF-6DB37A21F4D8.html (751 characters)\n",
      "Processed: B83B8F08-4193-49D2-8DF3-B49B85C27013.html (1380 characters)\n",
      "Processed: B88CBA2A-B79F-46AB-811C-90F425F0A982.html (428 characters)\n",
      "Processed: BA3C4606-9D72-4D23-A871-D75F70E1384D.html (1105 characters)\n",
      "Processed: BAB10324-43A2-4535-9D38-1440D1BAC907.html (400 characters)\n",
      "Processed: BB8A4F7F-D0A2-47C2-9A42-C9F56CCBA2A2.html (353 characters)\n",
      "Processed: BC1B7FC1-BB5F-4051-9BEF-17BD47B0243A.html (399 characters)\n",
      "Processed: BC378947-D57C-4739-9825-971E837AC292.html (1768 characters)\n",
      "Processed: BF488E8E-A352-4B79-9AB0-E4CE47A8A05D.html (322 characters)\n",
      "Processed: BF6B5636-C467-4A47-8FDE-2E3845DE0ED4.html (102 characters)\n",
      "Processed: BF77711D-C334-4222-97B6-5792FBEEE0C9.html (1364 characters)\n",
      "Processed: C057C704-F881-4B6C-9379-124C10BC642A.html (1681 characters)\n",
      "Processed: C0E628B3-6993-486C-8329-106D0DE0CDD7.html (367 characters)\n",
      "Processed: C296D76F-779C-4BD1-89E4-DA4BAB1DDA0A.html (385 characters)\n",
      "Processed: C33E5A46-8D82-4B34-A7D6-30DF8FC6C9BF.html (2114 characters)\n",
      "Processed: C3BC9141-E2C5-4B98-BEFD-8ACECF664656.html (518 characters)\n",
      "Processed: C3EC0FD4-3DEF-4061-B5DA-88B0AC8D3045.html (561 characters)\n",
      "Processed: C6DF3A55-4B06-4DD9-8490-83589460D60C.html (511 characters)\n",
      "Processed: CA4A2B77-38F8-470A-8051-A32F94936568.html (358 characters)\n",
      "Processed: CD4F0136-F289-4EF8-AEDE-4E4775BD6188.html (1681 characters)\n",
      "Processed: CEDDF5F5-44AE-40C9-A1EB-92A1E76921FD.html (1365 characters)\n",
      "Processed: CF108F38-C7D1-49EC-B31B-0F93361B0277.html (328 characters)\n",
      "Processed: D033D10B-C46D-47C4-8738-5CB472CE9D51.html (371 characters)\n",
      "Processed: D05300CA-A3E6-445D-A6C5-031E96B06462.html (1489 characters)\n",
      "Processed: D113CEEC-6ACE-485C-86C3-70D5F9560C5B.html (2880 characters)\n",
      "Processed: D189ECDC-6FC8-4D6E-BF36-7D69F606C2BD.html (360 characters)\n",
      "Processed: D422BF4E-E39D-4574-81EF-9DD06A976A07.html (321 characters)\n",
      "Processed: D44E3534-D6BC-4D61-86C9-4CFDDD3C5B1E.html (469 characters)\n",
      "Processed: D59CFA52-4EEF-4F5A-9E40-86F2C6D529F0.html (1447 characters)\n",
      "Processed: D91CFEBB-4C05-46E4-9EE3-C9223C13BD7D.html (1747 characters)\n",
      "Processed: DAB679C3-F82B-4887-B62C-8217D674A4D1.html (372 characters)\n",
      "Processed: DADB30B3-39B0-4DBE-BF5C-8ADAFEE2F403.html (2359 characters)\n",
      "Processed: E02ABED7-7666-4929-B314-FDFB59888CAA.html (411 characters)\n",
      "Processed: E07E3DD1-F469-4B6A-810B-35E702FE9C75.html (2980 characters)\n",
      "Processed: E11F7674-8E64-4758-91B0-A75C7FD289AB.html (282 characters)\n",
      "Processed: E1E04106-A6D8-4FFD-83E6-BA60522E865D.html (334 characters)\n",
      "Processed: E3D1115E-1E3E-4903-A606-9DAF5716A72D.html (378 characters)\n",
      "Processed: E4A86F2E-5660-4C50-A6C8-ACAD89FD555C.html (1768 characters)\n",
      "Processed: E4AA0DDB-9CA9-486B-A0A2-3B85E51E2B46.html (653 characters)\n",
      "Processed: E6501E73-D93D-48E4-81EB-FDA87C4E2784.html (564 characters)\n",
      "Processed: EAA77081-B239-4242-9385-4434580B5A3A.html (1362 characters)\n",
      "Processed: EAAAA32D-17A2-442E-B578-1F748045BCA2.html (368 characters)\n",
      "Processed: EACD8E7C-5FD2-4113-93D2-84BFACB11289.html (331 characters)\n",
      "Processed: ECAB6AEC-5B23-4C03-9B7B-C953B3415F72.html (389 characters)\n",
      "Processed: ED2485AB-EF00-4ED0-8B0B-7BF10E2726C8.html (330 characters)\n",
      "Processed: EF1DA9B3-5209-4FF0-8941-27A6E358E9BD.html (283 characters)\n",
      "Processed: EF514EC9-B2CF-4036-8F15-44A281C2749C.html (345 characters)\n",
      "Processed: F0FE11C6-F4CF-4268-8D29-3418A891D2FD.html (318 characters)\n",
      "Processed: F2056784-E639-4D60-B451-396A73386102.html (2525 characters)\n",
      "Processed: F31937DF-9C83-47FC-8067-6A231E9C129B.html (314 characters)\n",
      "Processed: F3C3C84F-8EB3-4006-8FD5-69443D576096.html (3414 characters)\n",
      "Processed: F675BDC8-7F1B-4793-A089-9507CFE4D374.html (330 characters)\n",
      "Processed: F6B2ECA7-2C67-4630-9021-1A8660A85BF7.html (2921 characters)\n",
      "Processed: F6E72578-7F52-4B73-834D-5278701645EB.html (2843 characters)\n",
      "Processed: F9B7C395-1411-4058-B1FD-F4F0D1A56876.html (1934 characters)\n",
      "Processed: FA130C8B-8129-4650-ACFB-80B477D7D7D0.html (505 characters)\n",
      "Processed: FACCEC3A-76E6-41BE-8619-38AD1DF05AB3.html (428 characters)\n",
      "Processed: FC27E53A-EF11-4F7E-AAE1-683281D928CC.html (494 characters)\n",
      "Processed: FC71243C-2F3B-44FF-A01A-9D1F20DC0E65.html (1936 characters)\n",
      "Processed: FCF02163-FCE0-4CDC-BB6D-285F4D0947BB.html (526 characters)\n",
      "Processed: FEB2F0A8-7C07-4E19-BFA9-824630032870.html (426 characters)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Create folder for cleaned text files\n",
    "os.makedirs(\"cleaned_text\", exist_ok=True)\n",
    "\n",
    "def html_to_text(html_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        \n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    text = soup.get_text()\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "pages_dir = \"pages\"\n",
    "documents = {}\n",
    "\n",
    "for filename in os.listdir(pages_dir):\n",
    "    if filename.endswith('.html'):\n",
    "        filepath = os.path.join(pages_dir, filename)\n",
    "        clean_text = html_to_text(filepath)\n",
    "        documents[filename] = clean_text\n",
    "        \n",
    "        text_filename = filename.replace('.html', '.txt')\n",
    "        with open(f\"cleaned_text/{text_filename}\", 'w', encoding='utf-8') as f:\n",
    "            f.write(clean_text)\n",
    "        \n",
    "        print(f\"Processed: {filename} ({len(clean_text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9233cfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved to index.json\n",
      "Total unique tokens: 3983\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize_with_positions(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    \n",
    "    token_positions = defaultdict(list)\n",
    "    for pos, token in enumerate(tokens):\n",
    "        token_positions[token].append(pos)\n",
    "    \n",
    "    return dict(token_positions)\n",
    "\n",
    "def build_positional_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        clean_doc_id = doc_id.replace('.html', '')\n",
    "        \n",
    "        token_positions = tokenize_with_positions(text)\n",
    "        \n",
    "        for token, positions in token_positions.items():\n",
    "            inverted_index[token].append([clean_doc_id, positions])\n",
    "    \n",
    "    return dict(inverted_index)\n",
    "\n",
    "inverted_index = build_positional_inverted_index(documents)\n",
    "\n",
    "# Save to JSON file with custom formatting\n",
    "with open('index.json', 'w', encoding='utf-8') as f:\n",
    "    f.write('{\\n')\n",
    "    items = list(inverted_index.items())\n",
    "    for i, (token, entries) in enumerate(items):\n",
    "        f.write(f'  \"{token}\": [\\n')\n",
    "        for j, entry in enumerate(entries):\n",
    "            entry_json = json.dumps(entry)\n",
    "            if j < len(entries) - 1:\n",
    "                f.write(f'    {entry_json},\\n')\n",
    "            else:\n",
    "                f.write(f'    {entry_json}\\n')\n",
    "        if i < len(items) - 1:\n",
    "            f.write('  ],\\n')\n",
    "        else:\n",
    "            f.write('  ]\\n')\n",
    "    f.write('}\\n')\n",
    "\n",
    "print(f\"Inverted index saved to index.json\")\n",
    "print(f\"Total unique tokens: {len(inverted_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf725890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 results for query: better to be\n",
      "\n",
      "1. Document: 4E638282-3800-49BA-A2C5-4E9BD45850AA\n",
      "   Cosine Similarity Score: 0.3424\n",
      "   TF-IDF Weights: better: 0.1825, to: 0.1587, be: 0.3121\n",
      "\n",
      "2. Document: EAAAA32D-17A2-442E-B578-1F748045BCA2\n",
      "   Cosine Similarity Score: 0.3424\n",
      "   TF-IDF Weights: better: 0.1825, to: 0.1587, be: 0.3121\n",
      "\n",
      "3. Document: 2A38CD67-658C-46EB-B447-BECD2960EC47\n",
      "   Cosine Similarity Score: 0.2161\n",
      "   TF-IDF Weights: better: 0.1055, to: 0.1071, be: 0.2105\n",
      "\n",
      "4. Document: 53477293-9075-4EF3-B04C-197B325F5411\n",
      "   Cosine Similarity Score: 0.2161\n",
      "   TF-IDF Weights: better: 0.1055, to: 0.1071, be: 0.2105\n",
      "\n",
      "5. Document: C057C704-F881-4B6C-9379-124C10BC642A\n",
      "   Cosine Similarity Score: 0.2161\n",
      "   TF-IDF Weights: better: 0.1055, to: 0.1071, be: 0.2105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the positional inverted index from index.json\n",
    "with open('index.json', 'r', encoding='utf-8') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "# Get all doc_ids from the index\n",
    "all_doc_ids = set()\n",
    "for token_entries in index.values():\n",
    "    for doc_id, _ in token_entries:\n",
    "        all_doc_ids.add(doc_id)\n",
    "all_doc_ids = list(all_doc_ids)\n",
    "\n",
    "def get_doc_text(doc_id):\n",
    "    filepath = f\"cleaned_text/{doc_id}.txt\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    return \"\"\n",
    "\n",
    "def search_tfidf(query_text, top_k):\n",
    "    # Use only doc_ids from index.json\n",
    "    doc_id_list = sorted(all_doc_ids)\n",
    "    corpus = [get_doc_text(doc_id) for doc_id in doc_id_list]\n",
    "    vectorizer = TfidfVectorizer(lowercase=True, token_pattern=r'\\b[a-z]+\\b')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    query_terms = [t for t in query_text.lower().split() if t.isalpha() or t.isalnum()]\n",
    "    query_string = ' '.join(query_terms)\n",
    "    query_vector = vectorizer.transform([query_string])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    results = []\n",
    "    for idx, doc_id in enumerate(doc_id_list):\n",
    "        score = similarities[idx]\n",
    "        tfidf_weights = {}\n",
    "        doc_vector = tfidf_matrix[idx]\n",
    "        for query_term in query_terms:\n",
    "            if query_term in feature_names:\n",
    "                term_idx = np.where(feature_names == query_term)[0]\n",
    "                tfidf_weights[query_term] = doc_vector[0, term_idx[0]] if len(term_idx) > 0 else 0.0\n",
    "            else:\n",
    "                tfidf_weights[query_term] = 0.0\n",
    "        results.append((doc_id, score, tfidf_weights))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# Example query\n",
    "query = \"better to be\"\n",
    "results = search_tfidf(query, top_k=5)\n",
    "print(f\"\\nTop {5} results for query: {query}\\n\")\n",
    "for rank, (doc_id, score, tfidf_weights) in enumerate(results, 1):\n",
    "    print(f\"{rank}. Document: {doc_id}\")\n",
    "    print(f\"   Cosine Similarity Score: {score:.4f}\")\n",
    "    print(f\"   TF-IDF Weights: {', '.join([f'{term}: {weight:.4f}' for term, weight in tfidf_weights.items()])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d93c9",
   "metadata": {},
   "source": [
    "### Step 3: Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample queries.csv created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "sample_queries = [\n",
    "    \"to be or not to be\",\n",
    "    \"We read to know we're not alone.\",\n",
    "    \"cup of tea large enough\"\n",
    "]\n",
    "\n",
    "with open('queries.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['query_id', 'query_text'])\n",
    "    for q in sample_queries:\n",
    "        writer.writerow([str(uuid.uuid4()).upper(), q])\n",
    "print(\"Sample queries.csv created.\")\n",
    "\n",
    "with open('queries.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    queries = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a43962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from index.json...\n",
      "✓ Index loaded: 3983 terms, 254 documents\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:26:48 [werkzeug] INFO: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://104.194.117.44:5000\n",
      "2025-12-02 15:26:48 [werkzeug] INFO: \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2025-12-02 15:26:54 [werkzeug] INFO: 127.0.0.1 - - [02/Dec/2025 15:26:54] \"POST /search/batch HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, send_file\n",
    "import os\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global variables\n",
    "all_doc_ids = None\n",
    "inverted_index = None\n",
    "\n",
    "\n",
    "def load_index_metadata(index_path='index.json'):\n",
    "    \"\"\"\n",
    "    Load index.json to get all document IDs\n",
    "    \"\"\"\n",
    "    global all_doc_ids, inverted_index\n",
    "    \n",
    "    print(f\"Loading index from {index_path}...\")\n",
    "    \n",
    "    with open(index_path, 'r', encoding='utf-8') as f:\n",
    "        inverted_index = json.load(f)\n",
    "    \n",
    "    # Extract all unique document IDs from the index\n",
    "    all_doc_ids = set()\n",
    "    for term, postings in inverted_index.items():\n",
    "        for doc_id, positions in postings:\n",
    "            all_doc_ids.add(doc_id)\n",
    "    \n",
    "    all_doc_ids = sorted(list(all_doc_ids))\n",
    "    \n",
    "    print(f\"✓ Index loaded: {len(inverted_index)} terms, {len(all_doc_ids)} documents\")\n",
    "    \n",
    "    return all_doc_ids\n",
    "\n",
    "\n",
    "def get_doc_text(doc_id):\n",
    "    \"\"\"\n",
    "    Load document text from cleaned_text directory\n",
    "    \"\"\"\n",
    "    file_path = os.path.join('cleaned_text', f'{doc_id}.txt')\n",
    "    if not os.path.exists(file_path):\n",
    "        return \"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def search_tfidf(query_text, top_k):\n",
    "    doc_id_list = sorted(all_doc_ids)\n",
    "    corpus = [get_doc_text(doc_id) for doc_id in doc_id_list]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(lowercase=True, token_pattern=r'\\b[a-z]+\\b')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    query_vector = vectorizer.transform([query_text.lower()])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    ranked_results = list(zip(doc_id_list, similarities))\n",
    "    ranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results[:top_k]\n",
    "\n",
    "\n",
    "def validate_query(query_text):\n",
    "    if not query_text or not isinstance(query_text, str):\n",
    "        return False, None, \"Query text is required\"\n",
    "    query_text = query_text.strip()\n",
    "    if len(query_text) == 0:\n",
    "        return False, None, \"Query cannot be empty\"\n",
    "    if len(query_text) > 500:\n",
    "        return False, None, \"Query too long\"\n",
    "    return True, query_text, None\n",
    "\n",
    "\n",
    "def parse_query_csv(file_content):\n",
    "    queries = []\n",
    "    csv_data = io.StringIO(file_content.decode('utf-8'))\n",
    "    reader = csv.DictReader(csv_data)\n",
    "\n",
    "    if 'query_id' not in reader.fieldnames or 'query_text' not in reader.fieldnames:\n",
    "        raise ValueError(\"CSV must contain query_id and query_text\")\n",
    "\n",
    "    for row in reader:\n",
    "        if not row['query_id'] or not row['query_text']:\n",
    "            continue\n",
    "        queries.append((row['query_id'].strip(), row['query_text'].strip()))\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return jsonify({\n",
    "        \"message\": \"Search Engine Query Processor\",\n",
    "        \"documents_loaded\": len(all_doc_ids) if all_doc_ids else 0,\n",
    "        \"endpoints\": {\n",
    "            \"/search/batch\": \"Upload CSV → download ranked CSV\",\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "@app.route('/search/batch', methods=['POST'])\n",
    "def search_batch_endpoint():\n",
    "    if all_doc_ids is None:\n",
    "        return jsonify({\"error\": \"Index not loaded\"}), 500\n",
    "    \n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"Missing CSV file\"}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    top_k = int(request.form.get('top_k', 3))\n",
    "    csv_content = file.read()\n",
    "\n",
    "    queries = parse_query_csv(csv_content)\n",
    "    results_out = []\n",
    "\n",
    "    for query_id, text in queries:\n",
    "        valid, cleaned, err = validate_query(text)\n",
    "        if not valid:\n",
    "            continue\n",
    "        \n",
    "        results = search_tfidf(cleaned, top_k)\n",
    "\n",
    "        for rank, (doc_id, score) in enumerate(results, 1):\n",
    "            results_out.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"rank\": rank,\n",
    "                \"document_id\": doc_id\n",
    "            })\n",
    "\n",
    "    # Create output CSV\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=[\"query_id\", \"rank\", \"document_id\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results_out)\n",
    "    output.seek(0)\n",
    "\n",
    "    return send_file(\n",
    "        io.BytesIO(output.getvalue().encode('utf-8')),\n",
    "        mimetype='text/csv',\n",
    "        as_attachment=True,\n",
    "        download_name=\"results.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Start server\n",
    "# --------------------------\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('index.json'):\n",
    "        print(\"ERROR: index.json not found!\")\n",
    "    else:\n",
    "        load_index_metadata()\n",
    "    \n",
    "    if not os.path.exists('cleaned_text'):\n",
    "        print(\"WARNING: cleaned_text directory missing!\")\n",
    "    \n",
    "    app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
