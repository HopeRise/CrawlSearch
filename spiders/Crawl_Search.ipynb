{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91425eb2",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This project focuses on designing and implementing a web-based search pipeline, including document \n",
    "acquisition, indexing, and query retrieval. A Scrapy-powered crawler is used to download and store web documents in \n",
    "HTML format. An inverted index is then generated using TF-IDF weighting to support efficient similarity-based \n",
    "retrieval. Finally, a Flask-driven query processor enables users to submit free-text searches and receive ranked \n",
    "document results based on cosine similarity.\n",
    "\n",
    "Future enhancements can focus on improving scalability and retrieval quality. Optional features such as concurrent \n",
    "and distributed crawling using AutoThrottle and Scrapyd would allow faster and broader document collection. Search \n",
    "accuracy could be increased by integrating semantic vector embeddings like Word2Vec or FAISS-based k-nearest neighbor \n",
    "similarity. Additionally, a front-end search interface and production deployment would make the system more \n",
    "user-accessible and robust.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project centers on web document retrieval and query processing. It is built around three key components:  web crawler, an indexer, and a query processor. Together, these components enable efficient crawling, indexing, and searching of web documents.\n",
    "\n",
    "### Solution Outline\n",
    "\n",
    "1. **Web Crawler**: Uses Scrapy to crawl the web and download web documents.\n",
    "2. **Indexer**: Builds an inverted index using TF-IDF and cosine similarity to support search and retrieval.\n",
    "3. **Query Processor**: Accepts user queries and returns ranked document results using TF-IDF and cosine similarity.\n",
    "\n",
    "### Relevant Literature: \n",
    "\n",
    "- [Scrapy Documentation](https://docs.scrapy.org/en/latest/intro/tutorial.html):  \n",
    "  This official tutorial provides a comprehensive introduction to Scrapy, a powerful Python framework for web crawling and scraping. It covers spider creation, data extraction, and best practices for scalable web data collection, which directly informs the design of the project's web crawler component.\n",
    "\n",
    "- [Flask Documentation](https://flask.palletsprojects.com/en/stable):  \n",
    "  The Flask documentation offers detailed guidance on building lightweight web applications and APIs in Python. It is the primary reference for implementing the query processor and REST API endpoints, enabling user interaction and search result delivery in this project.\n",
    "\n",
    "For the search indexing section, I have already studied and applied established techniques, as demonstrated in this project.\n",
    "\n",
    "### Proposed System\n",
    "\n",
    "The system uses the power of Scrapy for web crawling, Scikit-Learn for TF-IDF indexing and cosine similarity, and Flask for query processing. The combination of these technologies delivers a solution for web document retrieval and query processing.\n",
    "\n",
    "## Design\n",
    "\n",
    "Processing user queries involves three integrated components: the Scrapy crawler, which collects web documents; the indexer, which constructs the TF-IDF matrix; and the Flask-based processor, which handles incoming queries. These components interact through structured data exchange and well-defined interfaces to ensure smooth integration across the system.\n",
    "\n",
    "![Web Search Engine Diagram](Assets/Web_Search_Engine_Diagram.png)\n",
    "\n",
    "This activity diagram illustrates the sequence of operations within the system, including crawling, indexing, query processing, and delivering results. Each component works together to support the system’s overall functionality.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The system’s architecture is composed of three core components: the Scrapy crawler, the inverted index generator, and the Flask-based query processor. Together, these components facilitate web document crawling, indexing, and query handling.\n",
    "\n",
    "![Architecture Diagram](Assets/Architecture_Diagram.png)\n",
    "\n",
    "This diagram shows how the system’s components interact and communicate using interfaces such as file I/O and HTTP endpoints. The implementation leverages external libraries, including Scikit-Learn, BeautifulSoup, Scrapy, and Flask to provide the necessary functionality for crawling, parsing, indexing, and query processing.\n",
    "\n",
    "## Operation\n",
    "\n",
    "**Installation Instructions:**  \n",
    "  - Ensure Python 3.8 or higher is installed.  \n",
    "  - Create and activate a virtual environment:  \n",
    "    - `python -m venv venv`  \n",
    "    - `venv\\Scripts\\activate` (Windows) or `source venv/bin/activate` (Linux/Mac)  \n",
    "  - Install all required dependencies using:  \n",
    "    - `pip install -r requirements.txt`  \n",
    "\n",
    "**Software Commands:**  \n",
    "  - **Running the crawler:** Execute the relevant notebook cells or script to start the Scrapy crawler.  \n",
    "  - **Building the index:** Run the indexing cells or script to generate the inverted index (`index.json`).  \n",
    "  - **Running the Flask server and querying:** Start the Flask server by running the appropriate notebook cells or script. Submit queries via the web interface or batch CSV upload.\n",
    "\n",
    "**Inputs:**  \n",
    "  - **Seed URL:** The initial URL for the crawler.  \n",
    "  - **CSV queries:** A CSV file containing `query_id` and `query_text` columns for batch search.  \n",
    "  - **Configuration options:** Parameters such as maximum pages to crawl, crawl depth, and allowed domains.\n",
    "\n",
    "**Outputs:**  \n",
    "  - **Crawled HTML files:** Stored in the `pages/` directory.  \n",
    "  - **Cleaned text files:** Stored in the `cleaned_text/` directory.  \n",
    "  - **Inverted index:** Saved as `index.json`.  \n",
    "  - **Search results:** Ranked results provided as a downloadable CSV file via the Flask API.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- **Success/Failure:**  \n",
    "  The project successfully demonstrates the implementation of a modular web search pipeline, encompassing web crawling, document cleaning, inverted indexing, and query processing. The system is capable of acquiring web documents, extracting and cleaning their content, constructing a positional inverted index, and retrieving relevant documents in response to user queries using TF-IDF and cosine similarity. Error handling is incorporated to manage missing files or invalid queries, ensuring robust operation. Potential concerns include network interruptions, website changes, or anti-crawling measures that can disrupt document acquisition, as well as data quality issues or missing files that may affect indexing and search accuracy. Additionally, dependency mismatches, insufficient error handling, or scalability limitations could cause failures in processing or serving queries.\n",
    "\n",
    "- **Outputs:**  \n",
    "  The outputs of the system are as follows:  \n",
    "  - Crawled HTML files are stored in the `pages/` directory.  \n",
    "  - Cleaned text files are saved in the `cleaned_text/` directory.  \n",
    "  - The positional inverted index is generated and saved as `index.json`.  \n",
    "  - Search results for user queries are provided as ranked CSV files, which can be downloaded via the Flask API.  \n",
    "  - Example outputs, including ranked document results and similarity scores for sample queries, are displayed below as evidence of successful retrieval and ranking.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "- Web documents are sourced from [quotes.toscrape.com](https://quotes.toscrape.com)\n",
    "- Additional data sources can be integrated as needed for testing and evaluation.\n",
    "\n",
    "## Test Cases\n",
    "\n",
    "Test cases focus on verifying the functionality of the crawler, indexer, and query processor. Tools such as Scrapy’s built-in testing utilities and Python’s unit testing frameworks can be used to support this process. Test coverage includes scenarios related to crawling behavior, index generation, and query handling.\n",
    "\n",
    "## Source Code\n",
    "\n",
    "The complete source code for this project, including the web crawler, indexer, and query processor, is provided in the accompanying Jupyter notebook (`Crawl_Search.ipynb`) and related Python scripts within the project directory. All implementation details, code cells, and configuration files are included to enable full reproducibility of the results.\n",
    "\n",
    "Key files and directories:\n",
    "- `Crawl_Search.ipynb`: Main notebook containing the code for crawling, indexing, and query processing.\n",
    "- `pages/`: Directory where crawled HTML files are stored.\n",
    "- `cleaned_text/`: Directory containing cleaned text files extracted from HTML pages.\n",
    "- `index.json`: Generated inverted index used for search and retrieval.\n",
    "- `requirements.txt`: List of required Python packages for the project.\n",
    "\n",
    "To access or modify the source code, open the notebook or scripts in your preferred Python environment (e.g., Jupyter Notebook or Visual Studio Code).\n",
    "\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] Scrapy, “Scrapy Tutorial — Scrapy 2.3.0 documentation,” 2012. [Online]. Available: https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "[2] L. Richardson, “Beautiful Soup Documentation — Beautiful Soup 4.4.0 Documentation,” 2015. [Online]. Available: https://beautiful-soup-4.readthedocs.io/en/latest\n",
    "\n",
    "[3] W3Schools, “Python JSON,” 2025. [Online]. Available: https://www.w3schools.com/python/python_json.asp\n",
    "\n",
    "[4] Scikit-learn, “sklearn.metrics.pairwise.cosine_similarity,” 2018. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "[5] Scikit-learn, “TfidfVectorizer,” 2018. [Online]. Available: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "[6] Flask, “Welcome to Flask — Flask Documentation (3.1.x),” 2025. [Online]. Available: https://flask.palletsprojects.com/en/stable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d1e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6827908",
   "metadata": {},
   "source": [
    "### Web Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccbc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:23:35 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: scrapybot)\n",
      "2025-12-02 15:23:35 [scrapy.utils.log] INFO: Versions:\n",
      "{'lxml': '6.0.2',\n",
      " 'libxml2': '2.11.9',\n",
      " 'cssselect': '1.3.0',\n",
      " 'parsel': '1.10.0',\n",
      " 'w3lib': '2.3.1',\n",
      " 'Twisted': '25.5.0',\n",
      " 'Python': '3.14.0 (tags/v3.14.0:ebf955d, Oct  7 2025, 10:15:03) [MSC v.1944 '\n",
      "           '64 bit (AMD64)]',\n",
      " 'pyOpenSSL': '25.3.0 (OpenSSL 3.5.4 30 Sep 2025)',\n",
      " 'cryptography': '46.0.3',\n",
      " 'Platform': 'Windows-11-10.0.26200-SP0'}\n",
      "2025-12-02 15:23:35 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-12-02 15:23:35 [scrapy.extensions.telnet] INFO: Telnet Password: 30164f11bb523482\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-12-02 15:23:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 'INFO'}\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
      " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-12-02 15:23:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:23:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-12-02 15:23:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-12-02 15:23:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2025-12-02 15:23:38 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-12-02 15:23:38 [notebook_crawler] INFO: Total Documents Scraped: 254\n",
      "2025-12-02 15:23:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 129050,\n",
      " 'downloader/request_count': 323,\n",
      " 'downloader/request_method_count/GET': 323,\n",
      " 'downloader/response_bytes': 1361266,\n",
      " 'downloader/response_count': 323,\n",
      " 'downloader/response_status_count/200': 254,\n",
      " 'downloader/response_status_count/308': 69,\n",
      " 'dupefilter/filtered': 877,\n",
      " 'elapsed_time_seconds': 3.771481,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 12, 2, 21, 23, 38, 969220, tzinfo=datetime.timezone.utc),\n",
      " 'items_per_minute': 0.0,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 5,\n",
      " 'response_received_count': 254,\n",
      " 'responses_per_minute': 5080.0,\n",
      " 'scheduler/dequeued': 323,\n",
      " 'scheduler/dequeued/memory': 323,\n",
      " 'scheduler/enqueued': 323,\n",
      " 'scheduler/enqueued/memory': 323,\n",
      " 'start_time': datetime.datetime(2025, 12, 2, 21, 23, 35, 197739, tzinfo=datetime.timezone.utc)}\n",
      "2025-12-02 15:23:38 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"pages\", exist_ok=True)\n",
    "\n",
    "class NotebookCrawler(scrapy.Spider):\n",
    "    name = \"notebook_crawler\"\n",
    "    \n",
    "    def __init__(self, seed_url, allowed_domain, max_pages, max_depth, *args, **kwargs):\n",
    "        super(NotebookCrawler, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [seed_url]\n",
    "        self.allowed_domains = [allowed_domain]\n",
    "        self.max_pages = max_pages\n",
    "        self.visited = set()\n",
    "        \n",
    "        # Set max depth in custom settings\n",
    "        self.custom_settings = {\n",
    "            'DEPTH_LIMIT': max_depth,\n",
    "            'AUTOTHROTTLE_ENABLED': True,\n",
    "            'LOG_ENABLED': True,\n",
    "            'CLOSESPIDER_PAGECOUNT': max_pages\n",
    "        }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        page_uuid = str(uuid.uuid4()).upper() + \".html\"\n",
    "        \n",
    "        with open(f\"pages/{page_uuid}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        \n",
    "        self.visited.add(response.url)\n",
    "\n",
    "        # Continue crawling until max_pages reached\n",
    "        if len(self.visited) < self.max_pages:\n",
    "            links = LinkExtractor(allow_domains=self.allowed_domains).extract_links(response)\n",
    "            for link in links:\n",
    "                if link.url not in self.visited:\n",
    "                    yield response.follow(link.url, self.parse)\n",
    "\n",
    "    # Only print once when crawler is done\n",
    "    def closed(self, reason):\n",
    "        self.logger.info(f\"Total Documents Scraped: {len(self.visited)}\")\n",
    "\n",
    "\n",
    "# Crawler run\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'INFO',\n",
    "})\n",
    "\n",
    "process.crawl(\n",
    "    NotebookCrawler,\n",
    "    seed_url='https://quotes.toscrape.com',\n",
    "    allowed_domain='quotes.toscrape.com',\n",
    "    max_pages=100,\n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "try:\n",
    "    process.start()\n",
    "except:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972bb678",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a266a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 254 documents into cleaned_text folder.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Create folder for cleaned text files\n",
    "os.makedirs(\"cleaned_text\", exist_ok=True)\n",
    "\n",
    "def html_to_text(html_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        \n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    text = soup.get_text()\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text\n",
    "\n",
    "pages_dir = \"pages\"\n",
    "documents = {}\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(pages_dir):\n",
    "    if filename.endswith('.html'):\n",
    "        filepath = os.path.join(pages_dir, filename)\n",
    "        clean_text = html_to_text(filepath)\n",
    "        documents[filename] = clean_text\n",
    "        \n",
    "        text_filename = filename.replace('.html', '.txt')\n",
    "        with open(f\"cleaned_text/{text_filename}\", 'w', encoding='utf-8') as f:\n",
    "            f.write(clean_text)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "print(f\"\\nSuccessfully processed {count} documents into cleaned_text folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9233cfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index saved to index.json\n",
      "Total unique tokens: 3983\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def tokenize_with_positions(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b[a-z]+\\b', text)\n",
    "    \n",
    "    token_positions = defaultdict(list)\n",
    "    for pos, token in enumerate(tokens):\n",
    "        token_positions[token].append(pos)\n",
    "    \n",
    "    return dict(token_positions)\n",
    "\n",
    "def build_positional_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "    \n",
    "    for doc_id, text in documents.items():\n",
    "        clean_doc_id = doc_id.replace('.html', '')\n",
    "        \n",
    "        token_positions = tokenize_with_positions(text)\n",
    "        \n",
    "        for token, positions in token_positions.items():\n",
    "            inverted_index[token].append([clean_doc_id, positions])\n",
    "    \n",
    "    return dict(inverted_index)\n",
    "\n",
    "inverted_index = build_positional_inverted_index(documents)\n",
    "\n",
    "# Save to JSON file with custom formatting\n",
    "with open('index.json', 'w', encoding='utf-8') as f:\n",
    "    f.write('{\\n')\n",
    "    items = list(inverted_index.items())\n",
    "    for i, (token, entries) in enumerate(items):\n",
    "        f.write(f'  \"{token}\": [\\n')\n",
    "        for j, entry in enumerate(entries):\n",
    "            entry_json = json.dumps(entry)\n",
    "            if j < len(entries) - 1:\n",
    "                f.write(f'    {entry_json},\\n')\n",
    "            else:\n",
    "                f.write(f'    {entry_json}\\n')\n",
    "        if i < len(items) - 1:\n",
    "            f.write('  ],\\n')\n",
    "        else:\n",
    "            f.write('  ]\\n')\n",
    "    f.write('}\\n')\n",
    "\n",
    "print(f\"Inverted index saved to index.json\")\n",
    "print(f\"Total unique tokens: {len(inverted_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf725890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 results for query: better to be\n",
      "\n",
      "1. Document: 4E638282-3800-49BA-A2C5-4E9BD45850AA\n",
      "   Cosine Similarity Score: 0.3424\n",
      "   TF-IDF Weights: better: 0.1825, to: 0.1587, be: 0.3121\n",
      "\n",
      "2. Document: EAAAA32D-17A2-442E-B578-1F748045BCA2\n",
      "   Cosine Similarity Score: 0.3424\n",
      "   TF-IDF Weights: better: 0.1825, to: 0.1587, be: 0.3121\n",
      "\n",
      "3. Document: 2A38CD67-658C-46EB-B447-BECD2960EC47\n",
      "   Cosine Similarity Score: 0.2161\n",
      "   TF-IDF Weights: better: 0.1055, to: 0.1071, be: 0.2105\n",
      "\n",
      "4. Document: 53477293-9075-4EF3-B04C-197B325F5411\n",
      "   Cosine Similarity Score: 0.2161\n",
      "   TF-IDF Weights: better: 0.1055, to: 0.1071, be: 0.2105\n",
      "\n",
      "5. Document: C057C704-F881-4B6C-9379-124C10BC642A\n",
      "   Cosine Similarity Score: 0.2161\n",
      "   TF-IDF Weights: better: 0.1055, to: 0.1071, be: 0.2105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load the positional inverted index from index.json\n",
    "with open('index.json', 'r', encoding='utf-8') as f:\n",
    "    index = json.load(f)\n",
    "\n",
    "# Get all doc_ids from the index\n",
    "all_doc_ids = set()\n",
    "for token_entries in index.values():\n",
    "    for doc_id, _ in token_entries:\n",
    "        all_doc_ids.add(doc_id)\n",
    "all_doc_ids = list(all_doc_ids)\n",
    "\n",
    "def get_doc_text(doc_id):\n",
    "    filepath = f\"cleaned_text/{doc_id}.txt\"\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    return \"\"\n",
    "\n",
    "def search_tfidf(query_text, top_k):\n",
    "    # Use only doc_ids from index.json\n",
    "    doc_id_list = sorted(all_doc_ids)\n",
    "    corpus = [get_doc_text(doc_id) for doc_id in doc_id_list]\n",
    "    vectorizer = TfidfVectorizer(lowercase=True, token_pattern=r'\\b[a-z]+\\b')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    query_terms = [t for t in query_text.lower().split() if t.isalpha() or t.isalnum()]\n",
    "    query_string = ' '.join(query_terms)\n",
    "    query_vector = vectorizer.transform([query_string])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    results = []\n",
    "    for idx, doc_id in enumerate(doc_id_list):\n",
    "        score = similarities[idx]\n",
    "        tfidf_weights = {}\n",
    "        doc_vector = tfidf_matrix[idx]\n",
    "        for query_term in query_terms:\n",
    "            if query_term in feature_names:\n",
    "                term_idx = np.where(feature_names == query_term)[0]\n",
    "                tfidf_weights[query_term] = doc_vector[0, term_idx[0]] if len(term_idx) > 0 else 0.0\n",
    "            else:\n",
    "                tfidf_weights[query_term] = 0.0\n",
    "        results.append((doc_id, score, tfidf_weights))\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n",
    "\n",
    "# Example query\n",
    "query = \"better to be\"\n",
    "results = search_tfidf(query, top_k=5)\n",
    "print(f\"\\nTop {5} results for query: {query}\\n\")\n",
    "for rank, (doc_id, score, tfidf_weights) in enumerate(results, 1):\n",
    "    print(f\"{rank}. Document: {doc_id}\")\n",
    "    print(f\"   Cosine Similarity Score: {score:.4f}\")\n",
    "    print(f\"   TF-IDF Weights: {', '.join([f'{term}: {weight:.4f}' for term, weight in tfidf_weights.items()])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d93c9",
   "metadata": {},
   "source": [
    "### Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample queries.csv created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import uuid\n",
    "import random\n",
    "\n",
    "sample_queries = [\n",
    "    \"to be or not to be\",\n",
    "    \"We read to know we're not alone.\",\n",
    "    \"cup of tea large enough\"\n",
    "]\n",
    "\n",
    "with open('queries.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['query_id', 'query_text'])\n",
    "    for q in sample_queries:\n",
    "        writer.writerow([str(uuid.uuid4()).upper(), q])\n",
    "print(\"Sample queries.csv created.\")\n",
    "\n",
    "with open('queries.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    queries = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a43962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from index.json...\n",
      "✓ Index loaded: 3983 terms, 254 documents\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 15:26:48 [werkzeug] INFO: \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://104.194.117.44:5000\n",
      "2025-12-02 15:26:48 [werkzeug] INFO: \u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "2025-12-02 15:26:54 [werkzeug] INFO: 127.0.0.1 - - [02/Dec/2025 15:26:54] \"POST /search/batch HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, send_file\n",
    "import os\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global variables\n",
    "all_doc_ids = None\n",
    "inverted_index = None\n",
    "\n",
    "\n",
    "def load_index_metadata(index_path='index.json'):\n",
    "    \"\"\"\n",
    "    Load index.json to get all document IDs\n",
    "    \"\"\"\n",
    "    global all_doc_ids, inverted_index\n",
    "    \n",
    "    print(f\"Loading index from {index_path}...\")\n",
    "    \n",
    "    with open(index_path, 'r', encoding='utf-8') as f:\n",
    "        inverted_index = json.load(f)\n",
    "    \n",
    "    # Extract all unique document IDs from the index\n",
    "    all_doc_ids = set()\n",
    "    for term, postings in inverted_index.items():\n",
    "        for doc_id, positions in postings:\n",
    "            all_doc_ids.add(doc_id)\n",
    "    \n",
    "    all_doc_ids = sorted(list(all_doc_ids))\n",
    "    \n",
    "    print(f\"✓ Index loaded: {len(inverted_index)} terms, {len(all_doc_ids)} documents\")\n",
    "    \n",
    "    return all_doc_ids\n",
    "\n",
    "\n",
    "def get_doc_text(doc_id):\n",
    "    \"\"\"\n",
    "    Load document text from cleaned_text directory\n",
    "    \"\"\"\n",
    "    file_path = os.path.join('cleaned_text', f'{doc_id}.txt')\n",
    "    if not os.path.exists(file_path):\n",
    "        return \"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def search_tfidf(query_text, top_k):\n",
    "    doc_id_list = sorted(all_doc_ids)\n",
    "    corpus = [get_doc_text(doc_id) for doc_id in doc_id_list]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(lowercase=True, token_pattern=r'\\b[a-z]+\\b')\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    query_vector = vectorizer.transform([query_text.lower()])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    ranked_results = list(zip(doc_id_list, similarities))\n",
    "    ranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_results[:top_k]\n",
    "\n",
    "\n",
    "def validate_query(query_text):\n",
    "    if not query_text or not isinstance(query_text, str):\n",
    "        return False, None, \"Query text is required\"\n",
    "    query_text = query_text.strip()\n",
    "    if len(query_text) == 0:\n",
    "        return False, None, \"Query cannot be empty\"\n",
    "    if len(query_text) > 500:\n",
    "        return False, None, \"Query too long\"\n",
    "    return True, query_text, None\n",
    "\n",
    "\n",
    "def parse_query_csv(file_content):\n",
    "    queries = []\n",
    "    csv_data = io.StringIO(file_content.decode('utf-8'))\n",
    "    reader = csv.DictReader(csv_data)\n",
    "\n",
    "    if 'query_id' not in reader.fieldnames or 'query_text' not in reader.fieldnames:\n",
    "        raise ValueError(\"CSV must contain query_id and query_text\")\n",
    "\n",
    "    for row in reader:\n",
    "        if not row['query_id'] or not row['query_text']:\n",
    "            continue\n",
    "        queries.append((row['query_id'].strip(), row['query_text'].strip()))\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return jsonify({\n",
    "        \"message\": \"Search Engine Query Processor\",\n",
    "        \"documents_loaded\": len(all_doc_ids) if all_doc_ids else 0,\n",
    "        \"endpoints\": {\n",
    "            \"/search/batch\": \"Upload CSV → download ranked CSV\",\n",
    "        }\n",
    "    })\n",
    "\n",
    "\n",
    "@app.route('/search/batch', methods=['POST'])\n",
    "def search_batch_endpoint():\n",
    "    if all_doc_ids is None:\n",
    "        return jsonify({\"error\": \"Index not loaded\"}), 500\n",
    "    \n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({\"error\": \"Missing CSV file\"}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    top_k = int(request.form.get('top_k', 3))\n",
    "    csv_content = file.read()\n",
    "\n",
    "    queries = parse_query_csv(csv_content)\n",
    "    results_out = []\n",
    "\n",
    "    for query_id, text in queries:\n",
    "        valid, cleaned, err = validate_query(text)\n",
    "        if not valid:\n",
    "            continue\n",
    "        \n",
    "        results = search_tfidf(cleaned, top_k)\n",
    "\n",
    "        for rank, (doc_id, score) in enumerate(results, 1):\n",
    "            results_out.append({\n",
    "                \"query_id\": query_id,\n",
    "                \"rank\": rank,\n",
    "                \"document_id\": doc_id\n",
    "            })\n",
    "\n",
    "    # Create output CSV\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=[\"query_id\", \"rank\", \"document_id\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results_out)\n",
    "    output.seek(0)\n",
    "\n",
    "    return send_file(\n",
    "        io.BytesIO(output.getvalue().encode('utf-8')),\n",
    "        mimetype='text/csv',\n",
    "        as_attachment=True,\n",
    "        download_name=\"results.csv\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Start server\n",
    "# --------------------------\n",
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('index.json'):\n",
    "        print(\"ERROR: index.json not found!\")\n",
    "    else:\n",
    "        load_index_metadata()\n",
    "    \n",
    "    if not os.path.exists('cleaned_text'):\n",
    "        print(\"WARNING: cleaned_text directory missing!\")\n",
    "    \n",
    "    app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
